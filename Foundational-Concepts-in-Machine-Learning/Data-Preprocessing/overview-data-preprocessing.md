*Unintuitive effects and their consequences*. When one input to the multiply gate is very small and the other is very large, then the multiply gate will do something slightly unintuitive: it will assign a relatively huge gradient to the small input and a tiny gradient to the large input. Note that in linear classifiers where the weights are dot producted $w^Tx_i$ (multiplied) with the inputs, this implies that the scale of the data has an effect on the magnitude of the gradient for the weights. For example, If all input values $x_i$ are scaled by a factor of $1000$ during preprocessing, the gradients of the weights will also increase by a factor of $1000$. To stabilize training, youâ€™d have to lower the learning rate by that factor to compensate. This is why preprocessing matters a lot, sometimes in subtle ways! This highlights the importance of data preprocessing, such as normalization or standardization, to avoid unintended gradient scaling. Having an intuitive understanding of how gradients flow can help identify and resolve such issues during debugging.
