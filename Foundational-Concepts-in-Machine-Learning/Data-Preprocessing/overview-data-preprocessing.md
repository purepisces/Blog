_Unintuitive effects and their consequences_. When one input to the multiply gate is very small and the other is very large, the multiply gate behaves counterintuitively: it assigns a relatively huge gradient to the small input and a tiny gradient to the large input. However, in linear classifiers, this behavior is not inherently problematic, as larger inputs naturally correspond to larger weights, reflecting feature importance. For example, if all input values $x_i$ are scaled by a factor of $1000$ during preprocessing, the gradients of the weights will also increase by a factor of $1000$. To stabilize training, the learning rate would need to be scaled down accordingly.

This highlights the importance of data preprocessing, such as normalization or standardization, to avoid unintended gradient scaling. Proper preprocessing ensures gradients are neither excessively large (exploding gradients) nor extremely small (vanishing gradients). Having an intuitive understanding of how gradients flow helps identify and resolve such issues during debugging.

> **Unintended Gradient Scaling** refers to the phenomenon where the magnitude of gradients during backpropagation is unintentionally influenced by the scale of the input data. This can happen when input features are not properly normalized or standardized, leading to training instabilities."
